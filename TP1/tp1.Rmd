---
title: "Practical Session 1 - Modèles de Régression Linéaire"
author : "Richard CHEAM & Menghor THUO"
output:
  pdf_document:
    toc: true
    toc_depth: 3
  html_document: default
date: '03-10-2023'
header-includes:
  - \usepackage{bbold}
  - \usepackage{titlesec}
  - \usepackage {amsmath}
  - \newcommand{\sectionbreak}{\clearpage}
  #- \newcommand{\subsectionbreak}{\clearpage}
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
set.seed(123)
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())
graphics.off()
library(ggplot2)
```

# IV. Application: study your own data using a linear model with transformed data

## a) Retrieve data 

* To make sure the retrieved data is reliable, two different sites were referred to in order to compare each value obtained, precisely, year and revenue. As a result, the data were identical to one another.

* The data represents the growth of Amazon over the last 19 years, latest in 2022, based on the total amount of its income generated by the sale of goods and services (revenue) in $bn (billion-dollar).

* Amazon annual revenue data were retrieved from: 

(n.d.). Business of Apps - Connecting the app industry. https://www.businessofapps.com/data/amazon-statistics/

Amazon annual net sales 2022. (2023, February 3). Statista. https://www.statista.com/statistics/266282/annual-net-revenue-of-amazoncom/

## b) Organize data
```{r}
#saving data in a vector
year <- c(2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 
          2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022)
revenue <- c(6.92, 8.49, 10.71, 14.84, 19.17, 24.51, 34.2, 48.08, 61.09, 74.45, 
             88.99, 107.01, 135.99, 177.87, 232.89, 280.52, 386.06, 469.82, 513.98)

#using data.frame() to obtain a table and write it using write.table()
df_ <- data.frame(year, revenue)
write.table(df_, file='amazon.txt')

#read data using read.table()
tab_ <- read.table(file = 'amazon.txt', sep = " ", header = TRUE)

tab_
```

## c) Visualize data

* This step is crucial in order to have a good visual on the data set, its trend, pattern, and so on, so that it facilitates on the choice of model selection based on the underlying pattern of the data. Such approach can save time and prevent errors in the modeling process.

```{r}
ggplot(tab_, aes(x = year, y = revenue, color = "Data")) +
  geom_point() +
  labs(title = "Apple annual revenue 2003 to 2022", x = "Year", y = "Revenue in ($bn)")
```

* The graphic above shows the non-linear pattern (exponential growth), however, it is still appropriate to apply the logarithmic transformation. In this exercise, we will first fit the linear regression model, and then log-linear will be applied by taking the natural logarithm on the dependent variable (y) to turn the data into the linear pattern.

```{r}
ggplot(tab_, aes(x = year, y = log(revenue), color = "Data")) +
  geom_point()
```

## d) Linear model (bad model, just to see the difference)

```{r}
# using revenue y-dependent variable and year as x-explanary variable
model_ <- lm(revenue~., tab_)
summary(model_)
```
* The “year” coefficient has three asterisks (***) next to it.  This indicates that it is highly statistically significant, with a very small p-value of 2.58e-13, which is much less than 0.001.

* While the R-squared value (approaching 1) suggests a good relation between the explanatory variable (year) and independent variable (revenue), the residual standard error indicates a bad fit to the data set. 

```{r}
observed_ <- tab_$revenue
predicted_ <- predict(model_, tab_)
```

### d.1) Prediction study

```{r}
ggplot(data.frame(observed_, predicted_), aes(x = observed_, y = predicted_)) +
  geom_point(color = "cadetblue3") +
  geom_abline(intercept = 0, slope = 1, color = "brown1") +
  labs(title = "Observed vs Predicted Revenue in billions", x = "Observed Revenue", 
       y = "Predicted Revenue")
```
Visually, the data are not located on the bisector line indicating a great difference in value between the observed and predicted revenue in which we can conclude that the model is not good.

### d.2) Residual study

* In the case of a multiple regression model, since there are multiple predictors, we plot the residuals $\epsilon = y - y_i$ versus the predicted values $\hat{y_i}$. But, since this is a simple linear regression model, we instead plot the residuals versus the predictor $x_i$. 

```{r}
residuals_ <- model_$res
ggplot(data.frame(tab_$year, residuals_), aes(x = year, y = residuals_)) +
  geom_point(color = "brown3") +
  geom_hline(yintercept = 0, color = "green") +
  labs(title = "Residual Graph", x = "Year", y = "Residuals")
```
```{r}
sqrt((1/length(observed_)) * sum((observed_ - predicted_)^2))
```

* There is a systematic curvature in the residuals (U-shaped) which indicates that the assumed linear relationship between Y (revenue) and X (year) is not reasonable since there is still some information in the residuals. In addition, the RMSE of 70.22768 is quite high as well, hence, the model needs to be changed.

* So we will now look into another model which is a lot better for this trend.

## e) Log-linear model

* By applying the logarithmic transformation, we then obtained the following function: $$ ln(Y) = \beta_0 + \beta_1X + \epsilon$$

* where $X$ is the explanatory variable (year) and $Y$ is the response variable (revenue), and $\epsilon$ is the residual.

```{r}
#transform the y-dependent variable (revenue) using logarithm
logTab_ <- tab_
logTab_$revenue <- log(tab_$revenue)
#fit log-linear model using lm() function where year is the explanatory variable
logModel_ <- lm(revenue~., logTab_)
summary(logModel_)
```
* The “year” coefficient once again has a high statistical significance with an extremely small p-value < $(2)10^{-16}$, which is a lot less than 0.001.

* According to the summary of the model, it can be written as: $$ ln(Y) = -4.908(10^2) + 2.459(10^{-1})X + \epsilon$$

* For this model, we have a better $R^2 = 0.9963 \approx 1$ as it corresponds to the cosinus of the angle between the vector of the predicted value and the vector of the observed value, so the closer the angle approaches 0 (integer solution, $R^2 = cos^2\omega = 0.9963 \Rightarrow \omega \approx 0$), the better the model is. Plus, the residual standard is really low (0.08712) which suggests a high predictive accuracy.

### e.1) Prediction study

* Since this is log-linear model, we then would have a prediction function $\hat{y}$ as below: $$\hat{Y} = exp(\beta_0 + \beta_1X)$$

```{r}
logPredicted_ <- exp(predict(logModel_))

ggplot(data.frame(observed_, logPredicted_), aes(x = observed_, y = logPredicted_)) +
  geom_point(color = "cadetblue3") +
  geom_abline(intercept = 0, slope = 1, color = "brown1") +
  labs(title = "Observed vs. Predicted Revenue in billions", x = "Observed Revenue", 
       y = "Predicted Revenue")
```

* In conclusion, in accordance to the graphic above, we can see that the log-linear model, basically a logarithmic transformation of linear model, fitted the model better as the scatter points lie on the bisector line, which represents more accurately the growth trend of Amazon over the last 19 years. However, an outlier appeared in which we will see in the residual study below. 

### e.2) Residual study

```{r}
logResidual_ <- observed_ - logPredicted_

ggplot(data.frame(tab_$year, logResidual_), aes(x = year, y = logResidual_)) +
  geom_point(color = "brown3") +
  geom_hline(yintercept = 0) +
  labs(title = "Residual Graph", x = "Year", y = "Residuals")
```

* The points remain mostly on or near the horizontal line at zero (with one outlier), this can indicate that this model is doing a reasonably good job at capturing the relationship between the explanatory variable and the response variable, and on average, this model's predictions are reasonably close to the actual values. 

```{r}
logRMSE <- sqrt((1/length(observed_)) * sum((observed_ - logPredicted_)^2))
logRMSE
```
* With RMSE of 22.73713 and the presence of the single isolation in the residual plot, it suggests that this model has relatively good predictive performance but may make occasional errors.

# V. Cookies Study

```{r}
dataset <- read.csv("cookies.csv", header = TRUE)
dim(dataset)
```

* Indicating 32 observations with a $y$-dependent variable and $x_{i={1,2,...,700}}$-independent variables.

## a) Features extraction

```{r}
#separate response variable from explanatory variables
Y <- as.vector(dataset$fat)
X <- as.matrix(dataset[2:701])
```

```{r}
#calculate mean of each cookie by applying mean() function on each row of matrix X
mean_values <- apply(X , 1, mean)
mean_values
```

```{r}
#calculate standard deviation of each cookie
sd_values <- apply(X , 1, sd)
sd_values
```

```{r}
#calculate the minimum value of each cookie
minimun <- apply(X , 1, min)
minimun
```

```{r}
#calculate the maximum of each cookie
maximum <- apply(X , 1, max)
maximum
```

```{r}
#calculate slope for each cookie
slopes <- numeric(32)
x <- seq(1,700,1)
for (i in 1:32){
  mod <- lm(unlist(dataset[i, 2:701])~x)
slopes[i] <- coef(mod)[2]
}
slopes
```

## b) Regression model

```{r}
#create a data frame with 5 independent variables and fit linear model
data_tab <- data.frame(Y, mean_values, sd_values, minimun, maximum, slopes)
modreg <- lm(Y~., data=data_tab)
summary(modreg)
```
* The “slopes” coefficient has two asterisks (**) next to it. This indicates that it is moderately statistically significant, with a p-value of 0.00893, which is less than 0.01. While, the "standard deviation" has one asterisk (*) next to it meaning it is marginally statistically significant, with a p-value of 0.02430, which is less than less than 0.05. Therefore, the “slopes” variable is statistically significant at a significance level of 0.01, while the "sd" variable is at the significance level of 0.05.

* The model can be written as: $$Y = -2.728 + 9.563X_1 + 335.8X_2 + 0.5956X_3 - 3.338X_4 - 66140X_5 + \epsilon $$
```{r}
r_sqrt <- summary(modreg)$r.squared
r_sqrt
```

* The value $R^2 = 0.7166928$ is a positive indicator for this model in which it the predictor variables explain roughly 71.67% of the variance in the outcome we are trying to predict. However, other factors should still be considered in order to conclude whether this model fits well with the data set.

### b.1) Prediction study

```{r}
Y_hat <- predict(modreg, data_tab)

plot(Y, Y_hat,  pch = 19, col = 'cadetblue2', xlab = "Actual Values (Y)", 
     ylab = "Predicted Values (Y hat)", main = "Observed vs Predicted")
grid()
abline(lm(Y ~ Y_hat), col = 'green')
```

```{r}
rmse <- sqrt(mean((Y - Y_hat)^2))
rmse
```

* The RMSE value suggests a relatively small error which once again shows a good sign within the application of linear regression model. Nevertheless, based on the $(Y,\hat{Y})$ graph above, it seems like there are quite a decent amount of data points in which they are fairly far away from the bisector line. In the final analysis, the difference between the actual values and the predicted values demonstrate an inadequate quality of this model which we can conclude that this model is not that good.

### b.2) Residual study

```{r}
epsilon <- modreg$res

plot(Y_hat, epsilon, pch = 19, col = "red", xlab = "Predicted Values", 
     ylab = "Residuals", main = "Residual Plot For Linear Fit")
grid()
abline(0, 0)
```

* The $(\hat{\epsilon},\hat{y})$ plot shows no discernible pattern. Thus, there is no information to be captured for such random distribution of residual values.